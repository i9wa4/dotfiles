---
title: "How to train a linear regression model with dbt and BigFrames | dbt Developer Blog"
source_url: "https://docs.getdbt.com/blog/train-linear-dbt-bigframes"
fetched_at: "2025-12-16T14:20:18.809572+00:00"
---



On this page

## Introduction to dbt and BigFrames[‚Äã](#introduction-to-dbt-and-bigframes "Direct link to Introduction to dbt and BigFrames")

**dbt**: A framework for transforming data in modern data warehouses using modular SQL or Python. dbt enables data teams to develop analytics code collaboratively and efficiently by applying software engineering best practices such as version control, modularity, portability, CI/CD, testing, and documentation. For more information, refer to [What is dbt?](https://docs.getdbt.com/docs/introduction#dbt)

**BigQuery DataFrames (BigFrames)**: An open-source Python library offered by Google. BigFrames scales Python data processing by transpiling common Python data science APIs (pandas and scikit-learn) to BigQuery SQL.

You can read more in the [official BigFrames guide](https://cloud.google.com/bigquery/docs/bigquery-dataframes-introduction) and view the [public BigFrames GitHub repository](https://github.com/googleapis/python-bigquery-dataframes).

By combining dbt with BigFrames via the `dbt-bigquery` adapter (referred to as *"dbt-BigFrames"*), you gain:

* dbt‚Äôs modular SQL and Python modeling, dependency management with dbt.ref(), environment configurations, and data testing. With the cloud-based dbt platform, you also get job scheduling and monitoring.
* BigFrames‚Äô ability to execute complex Python transformations (including machine learning) directly in BigQuery.

`dbt-BigFrames` utilizes the **Colab Enterprise notebook executor service** in a GCP project to run Python models. These notebooks execute BigFrames code, which is translated into BigQuery SQL.

> Refer to these guides to learn more: [Use BigQuery DataFrames in dbt](https://cloud.google.com/bigquery/docs/dataframes-dbt) or [Using BigQuery DataFrames with dbt Python models](https://docs.getdbt.com/guides/dbt-python-bigframes?step=1).

To illustrate the practical impact of combining dbt with BigFrames, the following sections explore how this integration can streamline and scale a common machine learning task: training a linear regression model on large datasets.

## The power of dbt-BigFrames for large-scale linear regression[‚Äã](#the-power-of-dbt-bigframes-for-large-scale-linear-regression "Direct link to The power of dbt-BigFrames for large-scale linear regression")

Linear regression is a cornerstone of predictive analytics, used in:

* Sales forecasting
* Financial modeling
* Demand planning
* Real estate valuation

These tasks often require processing datasets too large for traditional in-memory Python. BigFrames alone solves this, but combining it with dbt offers a structured, maintainable, and production-ready way to train models or generate batch predictions on large data.

## ‚Äúdbt-BigFrames‚Äù with ML: A practical example[‚Äã](#dbt-bigframes-with-ml-a-practical-example "Direct link to ‚Äúdbt-BigFrames‚Äù with ML: A practical example")

We‚Äôll walk through training a linear regression model using a **dbt Python model powered by BigFrames**, focusing on the structure and orchestration provided by dbt.

We‚Äôll use the `epa_historical_air_quality` dataset from BigQuery Public Data (courtesy of the U.S. Environmental Protection Agency).

### Problem statement[‚Äã](#problem-statement "Direct link to Problem statement")

Develop a machine learning model to predict atmospheric ozone levels using historical air quality and environmental sensor data, enabling more accurate monitoring and forecasting of air pollution trends.

**Key stages:**

1. **Data Foundation**: Transform raw source tables into an analysis-ready dataset.
2. **Machine learning Analysis**: Train a linear regression model on the cleaned data.

## Setting up your dbt project for BigFrames[‚Äã](#setting-up-your-dbt-project-for-bigframes "Direct link to Setting up your dbt project for BigFrames")

### Prerequisites[‚Äã](#prerequisites "Direct link to Prerequisites")

* A Google Cloud account
* A dbt platform or Core setup
* Basic to intermediate SQL and Python
* Familiarity with dbt using [Beginner dbt guides](https://docs.getdbt.com/guides?level=Beginner)

### Sample `profiles.yml` for BigFrames[‚Äã](#sample-profilesyml-for-bigframes "Direct link to sample-profilesyml-for-bigframes")

```
my_epa_project:
  outputs:
    dev:
      compute_region: us-central1
      dataset: your_bq_dataset
      gcs_bucket: your_gcs_bucket
      location: US
      method: oauth
      priority: interactive
      project: your_gcp_project
      threads: 1
      type: bigquery
  target: dev
```

### Sample `dbt_project.yml`[‚Äã](#sample-dbt_projectyml "Direct link to sample-dbt_projectyml")

```
name: 'my_epa_project'
version: '1.0.0'
config-version: 2

models:
  my_epa_project:
    submission_method: bigframes
    notebook_template_id: 701881164074529xxxx  # Optional
    timeout: 6000
    example:
      +materialized: view
```

## The dbt Python models for linear regression[‚Äã](#the-dbt-python-models-for-linear-regression "Direct link to The dbt Python models for linear regression")

This project uses **two modular dbt Python models**:

1. `prepare_table.py` ‚Äî Ingests and prepares data
2. `prediction.py` ‚Äî Trains the model and generates predictions

### Part 1: Preparing the table (`prepare_table.py`)[‚Äã](#part-1-preparing-the-table-prepare_tablepy "Direct link to part-1-preparing-the-table-prepare_tablepy")

```
def model(dbt, session):
    dbt.config(submission_method="bigframes", timeout=6000)

    dataset = "bigquery-public-data.epa_historical_air_quality"
    index_columns = ["state_name", "county_name", "site_num", "date_local", "time_local"]
    param_column = "parameter_name"
    value_column = "sample_measurement"
    params_dfs = []

    table_param_dict = {
        "co_hourly_summary": "co",
        "no2_hourly_summary": "no2",
        "o3_hourly_summary": "o3",
        "pressure_hourly_summary": "pressure",
        "so2_hourly_summary": "so2",
        "temperature_hourly_summary": "temperature",
    }

    for table, param in table_param_dict.items():
        param_df = bpd.read_gbq(f"{dataset}.{table}", columns=index_columns + [value_column])
        param_df = param_df.sort_values(index_columns).drop_duplicates(index_columns).set_index(index_columns).rename(columns={value_column: param})
        params_dfs.append(param_df)

    wind_table = f"{dataset}.wind_hourly_summary"
    wind_speed_df = bpd.read_gbq(
        wind_table,
        columns=index_columns + [value_column],
        filters=[(param_column, "==", "Wind Speed - Resultant")]
    )
    wind_speed_df = wind_speed_df.sort_values(index_columns).drop_duplicates(index_columns).set_index(index_columns).rename(columns={value_column: "wind_speed"})
    params_dfs.append(wind_speed_df)

    df = bpd.concat(params_dfs, axis=1, join="inner").cache()
    return df.reset_index()
```

### Part 2: Training the model and making predictions (`prediction.py`)[‚Äã](#part-2-training-the-model-and-making-predictions-predictionpy "Direct link to part-2-training-the-model-and-making-predictions-predictionpy")

```
def model(dbt, session):
    dbt.config(submission_method="bigframes", timeout=6000)

    df = dbt.ref("prepare_table")

    train_data_filter = (df.date_local.dt.year < 2017)
    test_data_filter = (df.date_local.dt.year >= 2017) & (df.date_local.dt.year < 2020)
    predict_data_filter = (df.date_local.dt.year >= 2020)

    index_columns = ["state_name", "county_name", "site_num", "date_local", "time_local"]
    df_train = df[train_data_filter].set_index(index_columns)
    df_test = df[test_data_filter].set_index(index_columns)
    df_predict = df[predict_data_filter].set_index(index_columns)

    X_train, y_train = df_train.drop(columns="o3"), df_train["o3"]
    X_test, y_test = df_test.drop(columns="o3"), df_test["o3"]
    X_predict = df_predict.drop(columns="o3")

    from bigframes.ml.linear_model import LinearRegression
    model = LinearRegression()
    model.fit(X_train, y_train)
    df_pred = model.predict(X_predict)

    return df_pred
```

## Running your dbt ML pipeline[‚Äã](#running-your-dbt-ml-pipeline "Direct link to Running your dbt ML pipeline")

```
# Run all models
dbt run

# Or run just your new models
dbt run --select prepare_table prediction
```

## Key advantages of dbt and BigFrames for ML[‚Äã](#key-advantages-of-dbt-and-bigframes-for-ml "Direct link to Key advantages of dbt and BigFrames for ML")

* **Scalability & Efficiency**: Handle large datasets in BigQuery via BigFrames
* **Simplified Workflow**: Use familiar APIs like `pandas` and `scikit-learn`
* **dbt Orchestration**:
  + Dependency management with `dbt.ref()` and `dbt.source()`
  + Scheduled retraining with `dbt run`
  + Testing, documentation, and reproducibility

## Conclusion and next steps[‚Äã](#conclusion-and-next-steps "Direct link to Conclusion and next steps")

By integrating **BigFrames** into your **dbt workflows**, you can build scalable, maintainable, and production-ready machine learning pipelines. While this example used linear regression, the same principles apply across other ML use cases with `bigframes.ml`.

## Feedback and support[‚Äã](#feedback-and-support "Direct link to Feedback and support")

* üìö [dbt Support](https://docs.getdbt.com/docs/dbt-support)
* üì® Email feedback on BigFrames: [bigframes-feedback@google.com](mailto:bigframes-feedback@google.com)
* üõ† [File issues on GitHub](https://github.com/googleapis/python-bigquery-dataframes)
* üì¨ [Subscribe to BigFrames updates](https://docs.google.com/forms/d/10EnDyYdYUW9HvelHYuBRC8L3GdGVl3rX0aroinbRZyc/viewform?edit_requested=true)

#### Comments

![Loading](https://docs.getdbt.com/img/loader-icon.svg)

[Newer post

Building the Remote dbt MCP Server](https://docs.getdbt.com/blog/building-the-remote-dbt-mcp-server)[Older post

The new dbt VS Code extension: The experience we've all been waiting for](https://docs.getdbt.com/blog/vscode-extension-experience)
